{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, it will be explained how the csv.xz files were modified to eventually be meaningful for the research questions.\n",
    "The first part involved the loading of the necessary libraries, in particular pandas and lzma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "\n",
    "file_path320 = 'A320_valid.csv.xz'\n",
    "file_path319 = 'A319_valid.csv.xz'\n",
    "file_path321 = 'A321_valid.csv.xz'\n",
    "file_path332 = 'A332_valid.csv.xz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating all the columns and parameters in the excel file, we decided to include the following in our final datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the column to load\n",
    "columns_to_load = [\n",
    "    'time', \n",
    "    'timestep', \n",
    "    'maxtimestep', \n",
    "    'icao24', \n",
    "    'callsign', \n",
    "    'baroaltitude', \n",
    "    'lat', \n",
    "    'lon', \n",
    "    'velocity', \n",
    "    'segment', \n",
    "    'modeltype', \n",
    "    'operator', \n",
    "    'vertratecorr',\n",
    "    'fromICAO', \n",
    "    'toICAO', \n",
    "    'distance_from_dep', \n",
    "    'trip_distance', \n",
    "    'temp', \n",
    "    'tas'\n",
    "]\n",
    "\n",
    "# Load columns from the CSV\n",
    "with lzma.open(file_path319) as f:\n",
    "    df319 = pd.read_csv(f, usecols=columns_to_load)\n",
    "with lzma.open(file_path320) as f:\n",
    "    df320 = pd.read_csv(f, usecols=columns_to_load)\n",
    "with lzma.open(file_path321) as f:\n",
    "    df321 = pd.read_csv(f, usecols=columns_to_load)\n",
    "with lzma.open(file_path332) as f:\n",
    "    df332 = pd.read_csv(f, usecols=columns_to_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in section (region selection) only the flights in the area around paris are included. We therefore created a box based on minimum and maximum latitude and longitude.\n",
    "The box can be sign below. Afterwards we created a new dataframe which includes only the rows who at timestep 0 (start of measurement) are inside the given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data to only include flights in are\n",
    "lat_min = 48.65\n",
    "lat_max = 49.1\n",
    "lon_min = 2.01\n",
    "lon_max = 2.76\n",
    "\n",
    "filtered_df319 = df319[(df319['lat'] >= lat_min) & (df319['lat'] <= lat_max) & (df319['lon'] >= lon_min) & (df319['lon'] <= lon_max) & (df319['timestep'] == 0)]\n",
    "filtered_df320 = df320[(df320['lat'] >= lat_min) & (df320['lat'] <= lat_max) & (df320['lon'] >= lon_min) & (df320['lon'] <= lon_max) & (df320['timestep'] == 0)]\n",
    "filtered_df321 = df321[(df321['lat'] >= lat_min) & (df321['lat'] <= lat_max) & (df321['lon'] >= lon_min) & (df321['lon'] <= lon_max) & (df321['timestep'] == 0)]\n",
    "filtered_df332 = df332[(df332['lat'] >= lat_min) & (df332['lat'] <= lat_max) & (df332['lon'] >= lon_min) & (df332['lon'] <= lon_max) & (df332['timestep'] == 0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the column named segment can be used as a index for the flights, we created a list for each airplane, which only included the unique segments.\n",
    "We then took our previous dataframe and modified it to only include data about the flights whose segment is in the list created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique segments from the filtered dataframes and filter the original dataframes\n",
    "filtered_segments319 = filtered_df319['segment'].unique()\n",
    "df_filteredsegments319 = df319[df319['segment'].isin(filtered_segments319)]\n",
    "\n",
    "filtered_segments320 = filtered_df320['segment'].unique()\n",
    "df_filteredsegments320 = df320[df320['segment'].isin(filtered_segments320)]\n",
    "\n",
    "filtered_segments321 = filtered_df321['segment'].unique()\n",
    "df_filteredsegments321 = df321[df321['segment'].isin(filtered_segments321)]\n",
    "\n",
    "filtered_segments332 = filtered_df332['segment'].unique()\n",
    "df_filteredsegments332 = df332[df332['segment'].isin(filtered_segments332)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we wanted to have all flights comparable, we only included those, whose measurements starts between 0 and 100m. \n",
    "A similar approach above is used. First we created a dataframe which only included flights which at timestep 0 were below 100m.\n",
    "Then we created lists with the segments to be used for the indices. After comparing the original dataframe with the segments, we obtained the dataset which only included the flights starting below 100m.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data to only include flights below 100 at timestep 0\n",
    "filtered_heightdf319 = df_filteredsegments319[(df_filteredsegments319['baroaltitude'] <= 100) & (df_filteredsegments319['timestep'] == 0)]\n",
    "filtered_heightdf320 = df_filteredsegments320[(df_filteredsegments320['baroaltitude'] <= 100) & (df_filteredsegments320['timestep'] == 0)]\n",
    "filtered_heightdf321 = df_filteredsegments321[(df_filteredsegments321['baroaltitude'] <= 100) & (df_filteredsegments321['timestep'] == 0)]\n",
    "filtered_heightdf332 = df_filteredsegments332[(df_filteredsegments332['baroaltitude'] <= 100) & (df_filteredsegments332['timestep'] == 0)]\n",
    "\n",
    "# get the unique segments from the filtered dataframes\n",
    "plane_list319 = filtered_heightdf319['segment'].unique()\n",
    "plane_list320 = filtered_heightdf320['segment'].unique()\n",
    "plane_list321 = filtered_heightdf321['segment'].unique()\n",
    "plane_list332 = filtered_heightdf332['segment'].unique()\n",
    "\n",
    "final319 = df_filteredsegments319[df_filteredsegments319['segment'].isin(plane_list319)]\n",
    "final320 = df_filteredsegments320[df_filteredsegments320['segment'].isin(plane_list320)]\n",
    "final321 = df_filteredsegments321[df_filteredsegments321['segment'].isin(plane_list321)]\n",
    "final332 = df_filteredsegments332[df_filteredsegments332['segment'].isin(plane_list332)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to not store the modified dataset in csv files again, but instead opted for the use of pickle files. The reason therefore is the faster handling time and all our teammembers using python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the filtered dataframes to pickle filss\n",
    "final319.to_pickle('A319_final.pkl')\n",
    "final320.to_pickle('A320_final.pkl')\n",
    "final321.to_pickle('A321_final.pkl')\n",
    "final332.to_pickle('A332_final.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIL6022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
